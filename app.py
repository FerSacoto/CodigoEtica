import os
import gradio as gr

# Importaciones de LangChain y componentes
from langchain_community.document_loaders import UnstructuredWordDocumentLoader 
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings 
from langchain_community.vectorstores import FAISS 
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough 
from langchain_core.output_parsers import StrOutputParser

# -----------------------------------------------------------
# 1. VERIFICACI√ìN DE CLAVE API
# La clave GEMINI_API_KEY se carga autom√°ticamente desde los 'Secrets' del Space.
# -----------------------------------------------------------
if 'GEMINI_API_KEY' not in os.environ:
    # Esto detendr√° la ejecuci√≥n si la clave no est√° presente en el entorno de HF.
    raise ValueError("GEMINI_API_KEY no encontrada. Por favor, configura tu clave API en Hugging Face Secrets.")
print("‚úÖ Clave API cargada desde el entorno.")


# -----------------------------------------------------------
# 2. CARGA Y PROCESAMIENTO DEL DOCUMENTO DOCX (RAG Setup)
# -----------------------------------------------------------

file_path = "reglamento.docx" # Ruta relativa en el Space

try:
    # Carga robusta de DOCX (UnstructuredWordDocumentLoader)
    loader = UnstructuredWordDocumentLoader(file_path, mode="elements")
    pages = loader.load()
    
    if not pages:
        raise Exception("El cargador no extrajo contenido. El archivo DOCX puede estar vac√≠o o corrupto.")
        
    print(f"‚úÖ DOCX cargado exitosamente. Total de elementos extra√≠dos: {len(pages)}")

except FileNotFoundError:
    print(f"üö® ERROR CR√çTICO: No se encontr√≥ el archivo '{file_path}'. S√∫belo a tu Space.")
    raise 
except Exception as e:
    print(f"üö® ERROR AL CARGAR: {e}")
    raise
    
# Divisi√≥n (Chunking) con alta superposici√≥n
text_splitter = RecursiveCharacterTextSplitter(
    separators=["\n\n", "\n", ". ", " ", ""], # Separadores robustos
    chunk_size=2000, 
    chunk_overlap=400 # Overlap aumentado
)
docs = text_splitter.split_documents(pages)
print(f"‚úÖ Texto dividido en {len(docs)} trozos con alta superposici√≥n.")


# 3. CREACI√ìN DE EMBEDDINGS Y BASE DE DATOS FAISS
print("Creando embeddings y Base de Datos FAISS...")
embedding_model = HuggingFaceEmbeddings(
    model_name="all-MiniLM-L6-v2"
)

# Crear la Base de Datos con FAISS
db = FAISS.from_documents(docs, embedding_model)

# Definir el Retriever - M√°xima recuperaci√≥n
retriever = db.as_retriever(search_kwargs={"k": 20}) # k=20 para m√°xima recuperaci√≥n
print("‚úÖ Base de datos FAISS creada y Retriever listo (k=20).")


# 4. CONSTRUCCI√ìN DE LA CADENA RAG (LCEL)

# Inicializar el modelo Gemini
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash", 
    temperature=0.2, 
    google_api_key=os.environ['GEMINI_API_KEY'] 
)

# Definir el Prompt
prompt = ChatPromptTemplate.from_messages([
    ("system", 
      "ERES UN EXPERTO LEGAL UNIVERSITARIO y tu √öNICA fuente de respuesta es el CONTEXTO PROPORCIONADO. Responde formalmente y con precisi√≥n. Si el contexto NO contiene la respuesta, DEBES responder textualmente: 'Lo siento, no encuentro esa informaci√≥n espec√≠fica en el reglamento universitario. Por favor, consulta con la oficina correspondiente.'"),
    ("human", "CONTEXTO RECUPERADO:\n---\n{context}\n---\n\nPregunta del Usuario: {question}"),
])

# Funci√≥n para formatear documentos
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# Construir la Cadena RAG
qa_chain = (
    # 1. Recuperaci√≥n
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    # 2. Generaci√≥n
    | prompt
    | llm
    | StrOutputParser()
)


# 5. FUNCI√ìN Y INTERFAZ DE GRADIO
def rag_chat(user_input, chat_history):
    # Ignoramos chat_history, pero Gradio lo requiere en la firma de la funci√≥n
    try:
        answer = qa_chain.invoke(user_input)
        return answer
    except Exception as e:
        print(f"Error durante la invocaci√≥n de la cadena RAG: {e}")
        return "Hubo un error al procesar tu solicitud. Por favor, revisa los logs del servidor."


# Crear y Lanzar la Interfaz de Gradio
# ------------------------------------------------------------------
# CAMBIOS EST√âTICOS: Aplicamos un tema monocrom√°tico m√°s limpio.
# ------------------------------------------------------------------
demo = gr.ChatInterface(
    fn=rag_chat,
    chatbot=gr.Chatbot(
        height=450, 
        label="Asistente de Reglamento Universitario",
        # A√±adir un logo o icono de tu universidad (opcional: usando el par√°metro avatar_images)
    ),
    # T√≠tulo m√°s descriptivo y centrado
    title="üìö Asistente de Consulta: Reglamento Oficial", 
    
    # Usamos un tema diferente con un color primario para un toque moderno
    theme=gr.themes.Monochrome(primary_hue="blue", secondary_hue="cool"),
    
    description="Bienvenido al Asistente RAG. Haz preguntas espec√≠ficas sobre cualquier **art√≠culo, proceso o norma** contenida en el documento oficial (reglamento.docx).",
    
    # Ejemplos de preguntas revisados para ser m√°s representativos
    examples=[
        "¬øCu√°les son los requisitos de matr√≠cula para un estudiante nuevo?", 
        "¬øQu√© establece el reglamento sobre el procedimiento de apelaci√≥n de notas?", 
        "Detalles sobre las sanciones por fraude acad√©mico."
    ],
)

demo.queue()